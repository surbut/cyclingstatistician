\nonstopmode  % to allow pdflatex to compile even if errors are raised (e.g. missing figures)

\documentclass[10pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}
% \graphicspath{{./figures/}} % save all figures in the same directory
\usepackage{color} 
\usepackage{hyperref}
\usepackage{parskip}
\setlength{\parindent}{0pt}

%% PLEASE INCLUDE ALL MACROS BELOW
\newcommand{\Norm}{{\mathcal{N}}}
\newcommand{\BF}{{\text{BF}}}
\newcommand{\bma}{{\BF_\text{BMA}}}
\newcommand{\bmalite}{{\BF_\text{BMAlite}}}
\newcommand{\av}{\mbox{\boldmath$\alpha$}}
\newcommand{\dv}{\mbox{\boldmath$\delta$}}
\newcommand{\ddv}{\mbox{\boldmath$d$}}
\newcommand{\edv}{\mbox{\boldmath$\epsilon$}}
\newcommand{\bv}{\mbox{\boldmath$\beta$}}
\newcommand{\tauv}{\mbox{\boldmath$\tau$}}
\newcommand{\cv}{\mbox{\boldmath$c$}}
\newcommand{\bbv}{\tilde \bv}
\newcommand{\bev}{\mbox{\boldmath$b$}}
\newcommand{\ev}{\mbox{\boldmath$e$}}
\newcommand{\thv}{\mbox{\boldmath$\theta$}}
\newcommand{\tv}{\mbox{\boldmath$t$}}
\newcommand{\fv}{\mbox{\boldmath$f$}}
\newcommand{\Cv}{\mbox{\boldmath$C$}}
\newcommand{\Dv}{\mbox{\boldmath$D$}}
\newcommand{\Fv}{\mbox{\boldmath$F$}}
\newcommand{\gav}{\mbox{\boldmath$\gamma$}}
\newcommand{\Gav}{\mbox{\boldmath$\Gamma$}}
\newcommand{\Kv}{\mbox{\boldmath$K$}}
\newcommand{\iv}{\mbox{\boldmath$I$}}
\newcommand{\vv}{\mbox{\boldmath$v$}}
\newcommand{\pv}{\mbox{\boldmath$p$}}
\newcommand{\hv}{\mbox{\boldmath$h$}}
\newcommand{\gv}{\mbox{\boldmath$g$}}
\newcommand{\wv}{\mbox{\boldmath$w$}}
\newcommand{\Wv}{\mbox{\boldmath$W$}}
\newcommand{\Pv}{\mbox{\boldmath$P$}}
\newcommand{\Qv}{\mbox{\boldmath$Q$}}
\newcommand{\Rv}{\mbox{\boldmath$R$}}
\newcommand{\rv}{\mbox{\boldmath$r$}}
\newcommand{\sv}{\mbox{\boldmath$s$}}
\newcommand{\Sv}{\mbox{\boldmath$S$}}
\newcommand{\Sigv}{\mbox{\boldmath$\Sigma$}}
\newcommand{\qv}{\mbox{\boldmath$q$}}
\newcommand{\Mv}{\mbox{\boldmath$M$}}
\newcommand{\mv}{\mbox{\boldmath$\mu$}}
\newcommand{\mvg}{\mbox{\boldmath$\mu_g$}}
\newcommand{\Lv}{\mbox{\boldmath$L$}}
\newcommand{\lav}{\mbox{\boldmath$\lambda$}}
\newcommand{\Tv}{\mbox{\boldmath$T$}}
\newcommand{\Xv}{\mbox{\boldmath$X$}}
\newcommand{\xv}{\mbox{\boldmath$x$}}
\newcommand{\Uv}{\mbox{\boldmath$U$}}
\newcommand{\Vv}{\mbox{\boldmath$V$}}
\newcommand{\yv}{\mbox{\boldmath$y$}}
\newcommand{\yvg}{\mbox{\boldmath$y_g$}}
\newcommand{\Yv}{\mbox{\boldmath$Y$}}
\newcommand{\Zv}{\mbox{\boldmath$Z$}}
\newcommand{\zv}{\mbox{\boldmath$z$}}
\newcommand{\lv}{\bf{1}}
\newcommand{\etv}{\mbox{\boldmath$\eta$}}
\newcommand{\muLS}{\ensuremath{\hat{\mv}}}
\newcommand{\SigmaLS}{\ensuremath{\hat{\Sigma}}}
\newcommand{\fvPanel}{\ensuremath{\fv^{\rm panel}}}
\newcommand{\isa}{\ensuremath{\sigma_a^{-2}}}
\newcommand{\bfa}{\ensuremath{{\rm BF}}}
\newcommand{\hbfes}{\ensuremath{\widehat {\rm BF}^{\rm ES}}}
\newcommand{\hbfesmeta}{\ensuremath{\widehat {{\rm BF}}^{\rm ES}_{\rm meta}}}
\newcommand{\hbfesfix}{\ensuremath{\widehat {{\rm BF}}^{\rm ES}_{\rm fix}}}
\newcommand{\hbfesmax}{\ensuremath{\widehat {{\rm BF}}^{\rm ES}_{\rm maxH}}}
\newcommand{\hbfee}{\ensuremath{ \widehat {\rm BF}^{\rm EE} }}
\newcommand{\abfes}{\ensuremath{{\rm ABF^{ES}}}}
\newcommand{\abfee}{\ensuremath{{\rm ABF^{EE}}}}
\newcommand{\abfesc}{\ensuremath{{\rm A^*BF^{ES}}}}
%% END MACROS SECTION

\begin{document}

\title{Generative Model of Flutre, Wen, Pritchard and Stephens (2013)}
\author{Sarah Urbut}
\date{\today}

\maketitle

%\tableofcontents

\vspace{1cm}

This document describes the generative model of Flutre \textit{et al.} with more details and sometimes a slightly different notation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data}

Let's imagine we measured the expression level of $G$ genes in $S$ subgroups, e.g. tissues, from $N$ individuals which we also genotyped at $P$ genetic variants, typically SNPs.
The individuals are not necessarily present in all subgroups.

Using $g$ to index the genes, with $g \in \{1,\ldots,G\}$, we make two important assumptions about their expression levels.
We assume that the set of expression levels $\Yv_g$ of the $g$-th gene only depends on the genotypes at its $m_g$ \textit{cis} SNPs (with possibly other covariates).
We also assume that all genes are independent conditionally on these genotypes (and other covariates).

With $\Yv = (\Yv_1, \dots, \Yv_G)$ and $\Xv = (\Xv_1, \dots, \Xv_G)$ denoting the complete set of expression levels and genotypes, the observed log-likelihood of the whole data set can thus be written as
\begin{equation*}
  l(\Theta; \Yv | \Xv) = \log p(\Yv | \Xv, \Theta) = \sum_{g=1}^G \log p(\Yv_g | \Xv_g, \Theta)
\end{equation*}

where $\Theta$ denotes the set of all parameters detailed below.

For the moment we make no assumption as whether or not a gene is expressed in all subgroups, as this can be dealt with by preprocessing or by choosing a Poisson-like distribution.

Also, in practice, subgroup-specific confounding factors in $\Yv$ are regressed out beforehand, and the residuals, possibly quantile-normalized, are used as responses.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Likelihood of a gene-SNP pair}

The likelihood for gene $g$ and SNP $p$ is:

$Y_g \sim \mathcal{N}_{N \times S}(X_p B_{gp} + X_c B_{gc}, I_N, \Sigma_{gv})$

where:
\begin{itemize}
\item $Y_g$ is the $N \times S$ matrix of expression levels;
\item $\mathcal{N}_{N \times S}$ is the matrix-variate Normal distribution;
\item $X_p$ is the $N \times 1$ matrix of genotypes;
\item $B_{gp}$ is the $1 \times S$ matrix of genotypic effect sizes;
\item $X_c$ is the $N \times (1+Q)$ matrix of known covariates (including a column of 1's for the intercepts);
\item $B_{gc}$ is the $(1+Q) \times S$ matrix of covariate effect sizes (including the $\mu_s$);
\item $\Sigma_{gv}$ is the $S \times S$ covariance matrix of the errors.
\end{itemize}

For the uninitiated, let's consider how we might simulate the data with a specified matrix of SNPs and configurations. First, let's consider the error matrix $\mathbf \Sigma_{gv}$. For every level of gene expression measured for a set of $n$ (let's assume unrelated individuals), we have an  $N \times S$ matrix describing the residual errors remanning after the deterministic portion of the model has explained a proportion of the variation in gene expression measured on $S$ subgroups. Here, the $S$ subgroups represent tissue types, but we can imagine a situation in which we consider the effect of a particular SNP on disease risk . In attempting to simulate these levels of gene expression, we recognize that within a given individual $i$, the errors may be correlated. That is, there may be some covariance of gene expression between tissues for a given individual. We know that a vector of normals is distributed according to   is the $S \times S$ covariance matrix of the errors, $\mathbf \Sigma_{error}$. We know that the multivariate normal distribution of a k-dimensional random vector $X$ = $[X_{1}, X_{2}, …, X_{k}]$ can be written in the following notation:

\begin{equation}
\mathbf{x}\ \sim\ \mathcal{N}(\boldsymbol\mu,\, \boldsymbol\Sigma)
\end{equation}

In our case, this means that the vectors of residual errors for a given individual follows the multivariate normal distribution.  Because we assume that the individuals are unrelated, we assume that $(\epsilon_{1i},\dots,\epsilon_{Si})$ are independent and identically distributed as $\mathcal{N}(0, \mathbf \Sigma_gv)$. Now all that's left is to simulate $\mathbf \Sigma_{gv}$, the covariance matrix of the errors. For the moment, in the simulation data we imagine that every gene  possesses the same correlation matrix between tissues, and because the individuals are i.i.d their levels of gene expression between tissues are also distributed according to the same parameter. In order to simulate $\mathbf \Sigma_{gv}$, we take advantage of the following: 

In Bayesian statistics, in the context of the multivariate normal distribution, the Wishart distribution is the conjugate prior to the precision matrix $\omega$ = $\sigma^{-1}$, where $\mathbf \sigma_{gv}$ is the covariance matrix. If we are interested in simulating $\mathbf \Sigma_{gv}$, a reasonable choice is thus the inverse Wishart distribution.

Thus $\mathbf{X}={\mathbf \Sigma_{gv}}$ has an inverse Wishart distribution $\mathbf{X}\sim \mathcal{W}^{-1}({\mathbf\Sigma}^{-1},\nu)$

In our case, we use an independent inverse Wishart prior, with parameters $m$ (a positive scalar) and $\mathbf H$ (a positive-deﬁnite $r \times r$ matrix):

 $\mathbf \Sigma_{gv}}\ \sim \mathbf \mathcal{IW}({\mathbf \nu \mathbf{H}, $m$)$.
 
Here, $\nu$ is a function of the number of individuals minus the number of covariates , \mathbf H is the diagonal matrix of the identity matrix of the number of subgroups, and $m$ is also a function of the number of individuals.

So for every individual, we draw a vector of residual errors from the multivariate normal distribution with a covariance matrix drawn from the Inverse Wishart Distribution. We then stack these vectors of residual errosinto an $N \times S$ matrix, which is the same as drawing one $N \times S$ matrix from the matrix-variate normal distribution with the $S \times S$ covariance matrix $\mathbf \Sigma_{gv}$  and the $N \times N$ covariance matrix as the Identity matrix $\mathbf I_{nxn}$. If we assume that the the measurements on tissues are uncorrelated - perhaps they come from different individuals, we assume that it is an identity matrix, and thus specified with coverr=0. Thus
\begin{equation}
\mathbf{E} \ \sim MN(\mathbf 0, \mathbf I , \mathbf I)
\end{equation}

To summarize: 

When the tissue samples are taken from the same individuals we allow that the observations on the same individual may be correlated with one another.
Specifically, let $E := (\ev_1\,\cdots\,\ev_s)$  denote the $N \times S$ matrix of residual errors, the we assume it to follow a matrix-variate normal (MN) distribution, i.e.,
\begin{equation}
  \mathbf E \sim {\rm MN}(0, I, \mathbf \Sigma_{GV}).
\end{equation}    
That is, the vectors $(\epsilon_{1i},\dots,\epsilon_{Si})$ are independent and identically distributed as $\mathcal{N}(0, \Sigma)$.

Now we need to consider how we will simulate the effect size \bm{\beta}

A key component of our Bayesian model is the distribution $p(\bv | \gamma, \theta)$, where $\theta$ denotes hyper-parameters that are to be specified or estimated from the data. (In the main text we used $p(\beta | \gamma, \theta)$ to simplify exposition, but we actually work with the standardized effects $\bv$.) Of course, if $\gamma_s=0$ then
$b_s=0$ by definition. So it remains to specify the distribution of the 
remaining $b_s$ values for which $\gamma_s=1$.

We use the distribution from %\cite{Wen2011Bayesian} (see also \cite{Lebrec2010Dealing,Han2011RandomEffects})
which provides a flexible way to model the heterogeneity of genetic effects of an eQTL in multiple tissues.

Specifically, %\cite{Wen2011Bayesian}%

 consider a distribution $p(\bv | \phi, \omega, \gamma)$, with two hyper-parameters, $\phi,\omega$, 
in which the non-zero effects are normally
distributed about some mean $\bar b$, which itself is normally distributed:
\begin{equation}
  \label{het.prior}
  b_s  | \bar b, \gamma_s=1 \sim \mathcal{N}(\bar b, \phi^2),
\end{equation}    
and 
\begin{equation}
  \label{avg.prior}
  \bar b \sim \mathcal{N}(0, \omega^2).
\end{equation}
Note that $\phi^2+\omega^2$ controls the variance (and hence the
expected absolute size) of $b_s$, and $\phi^2/(\phi^2+\omega^2)$ controls
the heterogeneity (indeed, $\omega^2/(\phi^2+\omega^2)$ is the correlation of $b_s, b_{s'}$ for different subgroups $s \neq s'$). 
If $\phi^2=0$ then this model corresponds to the ``fixed effects" model in which the effects in all subgroups are equal (e.g. \cite{Han2012Interpreting}).

If we consider 
\begin{equation}
b_s  | \bar b, \gamma_s=1 \sim \mathcal{N}(\bar b, \phi^2),
\end{equation}    
 
 and we wish to determine the covariance matrix of the $b_s|\gamma_{1=1}, \gamma_{2=1}$, we can integrate out $\bar b$ using the following trick:

 \begin{array}
  \operatorname{E}[ \operatorname{E}[X \mid \mathcal{Y,Z}] \mid \mathcal{Y}] = \operatorname{E}[X \mid \mathcal{Y}].\\
  \operatorname{E}[ \operatorname{E}[b_{s1}, b_{s2} \mid  \bar b, \mathbf \gamma ] \mid \mathbf \gamma] = \operatorname{E}[b_{s1}, b_{s2}  \mid \mathcal{\gamma}].\\
\operatorname{E}[ \bar b^{2} \mid \mathbf \gamma] =  \omega^2\\
\end{array}

Similarly,

 \begin{array}
\operatorname{E}[ \operatorname{E}[b_{s1}^{2} \mid  \bar b, \mathbf \gamma ] \mid \mathbf \gamma] = \operatorname{E}[b_{s1}^{2} \mid \mathcal{\gamma}].\\
\operatorname{E}[ \phi^2 + \bar b^{2} \mid \mathbf \gamma] =  \omega^2 + \phi^2\\
\end{array}



Thus we can draw the $s$-component vector of effect sizes for a given gene-SNP pair with a specified effect from a multivariate normal,

\begin{equation}
\mathbf{b_s} | \gamma_s \sim \mathcal{N_{s}}(0, \mathbf{\Sigma_{g}})
\end{equation}

Where $\mathbf{\Sigma_{g}}$is an $S \times S$ matrix with $\omega^2 $+ $\phi^2$ on the diagonal  $\omega$ on the off diagonal.
 
%and 
%In the case of multiple SNPs, we vectorize the rows of $B_{gp}$ into $\bm{\beta}_{gp}$.
%Here, as we focus on one SNP at a time, we directly have $\bm{\beta}_{gp} = B_{gp}^T$.
%
%%As a prior, we use a mixture of multivariate Normals:
%%\[
%%p(\bm{\beta}_{gp}) = \sum_{l=1}^L \lambda_l \; p_l(\bm{\beta}_{gp}) \text{ so that } p_l(\bm{\beta}_{gp}) \text{ follows } \Norm_S(0, W_{0,l})
%%\]
%%The prior magnitude of genotypic effect sizes is parametrized by a grid of $L$ pairs $(\phi_l^2,\omega_l^2)$ so that $W_{0,l} = \phi_l^2 I_S + \omega_l^2 \bm{1}\bm{1}^T$.
%%The mixture proportions are uniformly equal a priori: $\lambda_l = 1/L$.
%%See Wen \& Stephens (2011) for the priors on the other parameters.


Then, to simulate data assuming at most one eQTN per gene, we draw a simulated configuration if the gene is indeed affected by an eQTN, and the gene expression $Y$ is simulated from a simple linear regression as:

$Y_g \sim \mathcal{N}_{N \times S}(X_p B_{gp} + X_c B_{gc}, I_N, \Sigma_{gv})$

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\section{Hierarchical model}
%
%For gene $g$, we use a latent binary indicator $z_g$ to denote if there is any eQTL in its \textit{cis}-region for any subgroup, in particular,
%\begin{equation}
%  \Pr(z_g = 1) = 1 - \pi_0.
%\end{equation}
%
%We use a latent indicator $m_g$-vector $\vv_g$ to denote the true eQTL (i.e. eQTN) conditional on $z_g=1$, and let $v_{gp}$ denote the $p$-th entry of $\vv_g$ ($p \in \{1,\ldots,m_g\}$).
%Here, our ``one \textit{cis} eQTL per gene'' assumption restricts $\vv_g$ to have at most one entry equal to 1, with the remaining entries being 0.
%Accordingly,
%\begin{equation*}
%  \Pr( \vv_g = {\bf 0}  | z_g = 0) = 1,
%\end{equation*}
%and, by ignoring genomic annotations, we also make the simplifying assumption that
%\begin{equation}
%  \Pr( v_{gp} = 1  | z_g = 1)  =  \frac{1}{m_g}.
%\end{equation}
%
%For gene $g$ and SNP $p$, we index all configurations and use a $(2^S-1)$-dimensional latent indicator vector $\cv_{gp}$ to denote the actual configuration for this gene-SNP pair.
%In case the SNP is not an eQTL,
%\begin{equation*}
%  \Pr( \cv_{gp} = {\bf 0}  | v_{gp} = 0) = 1.
%\end{equation*}
%
%Otherwise, we assume the gene-SNP pair belongs to the $k$-th configuration with prior probability
%\begin{equation}
%  \Pr( c_{gpk} = 1 | v_{gp} = 1) = \eta_k.
%\end{equation}
%with the constraints $\forall k \; \eta_k \ge 0$ and $\sum_k \eta_k = 1$.
%
%Joining the column vectors $\cv_{gp}$ for all $m_g$ SNPs, we obtain a latent $K \times m_g$ matrix $C_g$.
%
%Finally, given a grid of $L$ pairs of values $(\omega_l^2,\phi_l^2)$, we use the latent $L$-vector $\wv_{gp}$ to indicate the actual prior effect size for active subgroups for the pair of gene $g$ and SNP $p$.
%The $l$-th entry of this indicator vector is denoted by $w_{gpl}$, for which we assume prior probability
%\begin{equation*}
%  \Pr(\wv_{gp}= {\bf 0} | v_{gp} = 0 ) =  1,
%\end{equation*}
%and
%\begin{equation}
%  \Pr(w_{gpl}=1 | v_{gp} = 1 ) =  \lambda_l.
%\end{equation}
%with the constraints $\forall l \; \lambda_l \ge 0$ and $\sum_l \lambda_l = 1$.
%
%Joining the column vectors $\wv_{gp}$ for all $m_g$ SNPs, we obtain a latent $L \times m_g$ matrix $W_g$.
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\section{Augmented likelihood}
%
%In the maximum likelihood framework, we treat latent variables $z_g, \vv_g, C_g$ and $W_g$ as missing data.
%Let $\zv = (z_1,\dots,z_G)$, $\Vv = (\vv_1,\dots, \vv_G)$, $\Cv = (C_1,\dots,C_G)$ and $\Wv = (W_1,\dots W_G)$ denote the complete collection of latent variables.
%
%Let $\Theta=(\pi_0,\eta_1,\ldots,\eta_K,\lambda_1,\ldots,\lambda_L)$ denote the set of parameters.
%Based on the hierarchical model described in the previous section, we can now write out the augmented log-likelihood as follows,
%
%\begin{equation*}
%  \begin{aligned}
%    l_a(\Theta;\Yv,\zv,\Vv,\Cv,\Wv|\Xv) &= \sum_g \log p(\Yv_g,z_g,\vv_g,C_g,W_g | \Xv_g,\Theta)
%  \end{aligned}
%\end{equation*}
%
%Expanding the term inside the sum:
%\begin{equation*}
%  \begin{aligned}
%    \log p(\Yv_g,z_g,\vv_g,C_g,W_g | \Xv_g,\Theta) &= (1-z_g) \log \pi_0 + z_g \log (1-\pi_0) \\
%    &+ (1-z_g) \log p(\Yv_g|\Xv_g,z_g=0) + z_g \log p(\Yv_g,\vv_g,C_g,W_g | \Xv_g,\Theta,z_g=1) \\
%    &= (1-z_g) \log \pi_0 + z_g \log (1-\pi_0) + \log p(\Yv_g|\Xv_g,z_g=0) \\
%    &+ z_g \log \frac{p(\Yv_g,\vv_g,C_g,W_g | \Xv_g,\Theta,z_g=1)}{p(\Yv_g|\Xv_g,z_g=0)}
%  \end{aligned}
%\end{equation*}
%
%Expanding the last term:
%\begin{equation*}
%  \begin{aligned}
%    \log \frac{p(\Yv_g,\vv_g,C_g,W_g | \Xv_g,\Theta,z_g=1)}{p(\Yv_g|\Xv_g,z_g=0)} &= \sum_p v_{gp} \log \frac{1}{m_g} \\
%    &+ \sum_p v_{gp} \log \frac{p(\Yv_g,\cv_{gp},\wv_{gp} | \Xv_{gp},\Theta,v_{gp}=1)}{p(\Yv_g|\Xv_g,z_g=0)}
%  \end{aligned}
%\end{equation*}
%
%Expanding the last term inside the sum:
%\begin{equation*}
%  \begin{aligned}
%    \log \frac{p(\Yv_g,\cv_{gp},\wv_{gp} | \Xv_{gp},\Theta,v_{gp}=1)}{p(\Yv_g|\Xv_g,z_g=0)} &= \sum_k c_{gpk} \log \eta_k + \sum_l w_{gpl} \log \lambda_l \\
%    &+ \sum_k \sum_l c_{gpk} w_{gpl} \log \frac{p(\Yv_g | \Xv_{gp},\Theta,c_{gpk}=1,w_{gpl}=1)}{p(\Yv_g|\Xv_g,z_g=0)}
%  \end{aligned}
%\end{equation*}
%
%where $\BF_{gpkl} = \frac{p(\Yv_g | \Xv_{gp},\Theta,c_{gpk}=1,w_{gpl}=1)}{p(\Yv_g|\Xv_g,z_g=0)}$ and is usually pre-computed.
%
%Putting everything back together:
%\begin{equation}
% \label{laug}  
%  \begin{aligned}
%    l_a(\Theta;\Yv,\zv,\Vv,\Cv,\Wv|\Xv) &= \sum_g (1-z_g)\log \pi_0 + \sum_g z_g \log (1-\pi_0) + \sum_g \log p(\Yv_g|\Xv_g,z_g=0) \\
%    &+ \sum_{g,p} z_gv_{gp} \log \frac{1}{m_g} + \sum_{g,p,k}z_g v_{gp} c_{gpk} \log \eta_k + \sum_{g,p,l} z_g v_{gp} w_{gpl} \log \lambda_l  \\
%    &+ \sum_{g,p,k,l} z_g v_{gp} c_{gpk} w_{gpl} \log \BF_{gpkl}
%  \end{aligned}
%\end{equation}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\section{EM algorithm}
%
%The EM algorithm searches for the maximum likelihood estimate of $\Theta$, by iteratively performing an expectation (E) step and a maximization (M) step of the following objective function, noted $Q$:
%\begin{equation}
%  \label{Q}
%  \begin{aligned}
%    Q(\Theta|\Yv,\Xv,\Theta^{(j)}) = {\rm E}_{\zv,\Vv,\Cv,\Wv|\Yv,\Xv,\Theta} [ l_a(\Theta) | \Yv,\Xv,\Theta^{(j)} ]
%  \end{aligned}
%\end{equation}
%
%Starting from randomly-initialized parameters $\Theta^{(0)}$, in the E-step for the $(j+1)^{\text{th}}$ iteration, we evaluate the objective function (\ref{Q}), i.e. the conditional expectation over the latent variables of the augmented log-likelihood (\ref{laug}) given the observed data $\Xv$ and $\Yv$ and the current estimates of the parameters $\Theta^{(j)}$.
%
%\begin{equation}
%  \begin{aligned}
%    {\rm E}[z_g | \Yv, \Xv, \Theta^{(j)}] &= \Pr(z_g=1 | \Yv, \Xv,\Theta^{(j)}) \\ 
%    &= \frac{\Pr(z_g=1 | \Theta^{(j)}) \cdot p(\Yv | \Xv, \Theta^{(j)}, z_g=1)}{p(\Yv | \Xv, \Theta^{(j)})} \\
%    &= \frac{\Pr(z_g=1 | \Theta^{(j)}) \cdot \prod_{g'} p(\Yv_{g'} | \Xv_{g'}, \Theta^{(j)}, z_g=1)}{\prod_{g'} p(\Yv_{g'} | \Xv_{g'}, \Theta^{(j)})} \\
%    &= \frac{\Pr(z_g=1 | \Theta^{(j)}) \cdot p(\Yv_g | \Xv_g, \Theta^{(j)}, z_g=1)}{p(\Yv_g | \Xv_g, \Theta^{(j)})} \\
%    &= \frac{(1-\pi_0^{(j)}) \BF^{(j)}_{g}}{\pi_0^{(j)} + (1-\pi_0^{(j)}) \BF^{(j)}_{g}}.
%  \end{aligned}
%\end{equation}
%
%where 
%\begin{equation*}
%  \begin{aligned}
%    \BF^{(j)}_{g} &= \frac{p(\Yv_g | \Xv_g, \Theta^{(j)}, z_g=1)}{p(\Yv_g | \Xv_g, z_g=0)} \\
%    &= \sum_{p,k,l} \frac{1}{m_g} \eta_k^{(j)} \lambda_l^{(j)} \BF_{gpkl},
%  \end{aligned}
%\end{equation*}
%
%Similarly,
%\begin{equation}
%  \begin{aligned}
%    {\rm E}[z_g v_{gp} | \Yv, \Xv, \Theta^{(j)}] &= \frac{(1-\pi_0^{(j)}) \frac{1}{m_g} \BF^{(j)}_{gp}}{\pi_0^{(j)} + (1-\pi_0^{(j)}) \BF^{(j)}_{g}},
%  \end{aligned}
%\end{equation}
%
%where 
%\begin{equation*}
%  \begin{aligned}
%    \BF^{(j)}_{gp} &= \frac{p(\Yv_g | \Xv_{gp}, \Theta^{(j)}, z_g=1, v_{gp}=1)}{p(\Yv_g | \Xv_g, z_g=0)} \\
%    &=\sum_{k,l} \eta_k^{(j)} \lambda_l^{(j)} \BF_{gpkl}.
%  \end{aligned}
%\end{equation*}
%
%And
%\begin{equation}
%  \begin{aligned}
%    {\rm E}[z_g v_{gp} c_{gpk} | \Yv, \Xv, \Theta^{(j)}] &= \frac{(1-\pi_0^{(j)}) \frac{1}{m_g} \eta_k^{(j)} \sum_l \lambda_l^{(j)} \BF_{gpkl}}{\pi_0^{(j)} + (1-\pi_0^{(j)}) \BF^{(j)}_{g}},
%  \end{aligned}
%\end{equation}
%
%\begin{equation}
%  \begin{aligned}
%    {\rm E}[z_g v_{gp} w_{gpl} | \Yv, \Xv, \Theta^{(j)}] &= \frac{(1-\pi_0^{(j)}) \frac{1}{m_g} w_l^{(j)} \sum_k \eta_k^{(j)} \BF_{gpkl}}{\pi_0^{(j)} + (1-\pi_0^{(j)}) \BF^{(j)}_{g}}.
%  \end{aligned}
%\end{equation}
%
%
%In the M-step for the $(j+1)^{\text{th}}$ iteration, we estimate a new set of parameters, $\Theta^{(j+1)}$, by maximizing the objective function (\ref{Q}), i.e. the conditional expectation over the latent variables of the augmented log-likelihood (\ref{laug}) given the observed data $\Xv$ and $\Yv$ and the current estimates of the parameters $\Theta^{(j)}$.
%
%In particular, for $\pi_0$,
%\begin{equation*}
%  \begin{aligned}
%    \frac{\partial Q}{\partial \pi_0}(\pi_0) &= \sum_g \left[ (1-{\rm E}[z_g | \Yv, \Xv, \Theta^{(j)}]) \times \frac{1}{\pi_0} \right] - \sum_g \left[ {\rm E}[z_g | \Yv, \Xv, \Theta^{(j)}] \times \frac{1}{1-\pi_0} \right],
%  \end{aligned}
%\end{equation*}
%
%\begin{equation*}
%  \begin{aligned}
%    \frac{\partial Q}{\partial \pi_0}(\pi_0^{(j+1)}) = 0 \Leftrightarrow \pi_0^{(j+1)} = \frac{1}{G} \sum_g (1 - {\rm E}[z_g | \Yv, \Xv, \Theta^{(j)}]),
%  \end{aligned}
%\end{equation*}
%
%which gives
%\begin{equation}
%  \begin{aligned}
%    \pi_0^{(j+1)} = \frac{1}{G} \sum_g \frac{\pi_0^{(j)}}{\pi_0^{(j)} + (1-\pi_0^{(j)}) \BF_g^{(j)}}.
%  \end{aligned}
%\end{equation}
%
%Now for the grid points, using a Lagrange multiplier, $L_a$, to enforce the constraints,
%\begin{equation*}
%  \begin{aligned}
%    \frac{\partial Q}{\partial \lambda_l}(\lambda_l) &= \sum_{g,p} \left[ {\rm E}[z_g v_{gp} w_{gpl} | \Yv, \Xv, \Theta^{(j)}] \times \frac{1}{\lambda_l} \right] - L_a,
%  \end{aligned}
%\end{equation*}
%
%\begin{equation*}
%  \begin{aligned}
%    \frac{\partial Q}{\partial \lambda_l}(\lambda_l^{(j+1)}) = 0 \Leftrightarrow \lambda_l^{(j+1)} = \frac{1}{L_a} \sum_{g,p} {\rm E}[z_g v_{gp} w_{gpl} | \Yv, \Xv, \Theta^{(j)}],
%  \end{aligned}
%\end{equation*}
%
%which gives
%\begin{equation}
%  \begin{aligned}
%    \lambda_l^{(j+1)} = \frac{\sum_{g,p,k} \frac{\frac{1}{m_g} \eta_k^{(j)} \BF_{gpkl}}{\pi_0^{(j)} + (1-\pi_0^{(j)}) \BF_g^{(j)}} \cdot \lambda_l^{(j)}}{\sum_{l'} \left( \sum_{g,p,k} \frac{\frac{1}{m_g} \eta_k^{(j)} \BF_{gpkl'}}{\pi_0^{(j)} + (1-\pi_0^{(j)}) \BF_g^{(j)}} \cdot \lambda_{l'}^{(j)} \right)}.
%  \end{aligned}
%\end{equation}
%
%where the $(1-\pi_0^{(j)})$ simplified but the $1/m_g$ are kept as the SNP prior could easily become SNP-specific via the usage of external information such as genome annotations.
%
%Finally, for the configurations,
%\begin{equation}
%  \begin{aligned}
%    \eta_k^{(j+1)} = \frac{\sum_{g,p,l} \frac{\frac{1}{m_g} \lambda_l^{(j)} \BF_{gpkl}}{\pi_0^{(j)} + (1-\pi_0^{(j)}) \BF_g^{(j)}} \cdot \eta_k^{(j)}}{\sum_{k'} \left( \sum_{g,p,l} \frac{\frac{1}{m_g} \lambda_l^{(j)} \BF_{gpkl}}{\pi_0^{(j)} + (1-\pi_0^{(j)}) \BF_g^{(j)}} \cdot \eta_{k'}^{(j)} \right)}.
%  \end{aligned}
%\end{equation}
%
%
%We initiate the EM algorithm by setting $\Theta^{(0)}$ to initial values, %some random values,
%and run iterations until some pre-defined convergence threshold is met.
%In practice, we monitor the monotonic increase of the observed log-likelihood between successive iterations, and stop the iterations as the increment becomes sufficiently small (e.g. 0.05).
%
%We can also construct profile likelihood confidence intervals for $\hat{\Theta}$.
%For example, a $(1-\alpha)\%$ profile likelihood confidence set for $\pi_0$ is built as 
%\begin{equation}
%  \{\pi_0: \log p(\Yv | \Xv, \pi_0, \hat \etv, \hat \lav) > \log p(\Yv | \Xv, \hat \pi_0, \hat \etv, \hat \lav) - \frac{1}{2}Z^2_{(1-\alpha)} \},
%\end{equation}
%where $\hat \pi_0, \hat \etv, \hat \lav$ are the MLEs obtained from the EM algorithm. 
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\section{Posteriors on latent variables}
%
%Once the hyperparameters have been estimated, several posteriors of interest can be calculated.
%The first is the posterior probability that the gene has one eQTL in at least one subgroup:
%\begin{equation*}
%  \begin{aligned}
%    \Pr(z_g = 1 | \Yv, \Xv, \Theta) = \frac{(1-\hat{\pi}_0) \BF_g}{\hat{\pi}_0 + (1-\hat{\pi}_0) \BF_g}
%  \end{aligned}
%\end{equation*}
%
%Note that the $\BF_g$'s alone can be used in order to estimate $\pi_0$ via the EBF procedure (X. Wen, personal communication).
%
%At the SNP level, the posterior probability that the SNP is ``the'' eQTL for the gene, in at least one subgroup, given that the gene has exactly one eQTL, assuming all cis SNPs are equally likely, is given by:
%\begin{equation}
%  \begin{aligned}
%    \Pr(v_{gp} = 1 | \Yv, \Xv, \Theta, z_g = 1) &= \frac{\frac{1}{m_g} p(Y_g | X_g, \Theta, z_g=1, v_{gp}=1)}{p(Y_g | X_g, \Theta, z_g = 1)} \\
%    &= \frac{\frac{1}{m_g} \BF_{gp}}{\BF_g} = \frac{\BF_{gp}}{\sum_p \BF_{gp}}
%  \end{aligned}
%\end{equation}
%
%In order to make results more comparable with models only considering the SNP level, the posterior probability that the SNP is ``an'' eQTL for the gene, in at least one subgroup, given that all SNPs are independent (and effects are small), can be written as:
%\begin{equation}
%  \begin{aligned}
%    \Pr(v_{gp} = 1 | \Yv, \Xv, \Theta, z'_g \ge 1) &= \frac{\frac{1}{m_g} \BF_{gp}}{\frac{1}{m_g} \BF_{gp} + (1-\frac{1}{m_g})}
%  \end{aligned}
%\end{equation}
%
%But of most interest is the posterior probability that the SNP is an eQTL in a given subgroup, given that it is ``the'' eQTL for the gene:
%\begin{equation}
%  \begin{aligned}
%    \Pr(\gamma_{gps} = 1 | \Yv, \Xv, \Theta, z_g = 1, v_{gp} = 1) &= \sum_{k:\gamma_{gps}=1|c_{gpk}} \Pr(c_{gpk} = 1 | \Yv, \Xv, \Theta, z_g = 1, v_{gp} = 1) \\
%    &= \sum_{k:\gamma_{gps}=1 | c_{gpk}} \frac{\hat{\eta}_k \sum_l \hat{\lambda}_l \BF_{gpkl}}{\BF_{gp}} = \sum_{k:\gamma_{gps}=1 | c_{gpk}} \frac{\hat{\eta}_k \sum_l \hat{\lambda}_l \BF_{gpkl}}{\sum_{k',l} \hat{\eta}_{k'} \hat{\lambda}_l \BF_{gpk'l}}
%  \end{aligned}
%\end{equation}
%
%When reporting the results, it is usually better to report the following marginal posterior:
%\begin{equation*}
%  \begin{aligned}
%    \Pr(\gamma_{gps} = 1 | \Yv, \Xv, \Theta) &= \Pr(\gamma_{gps} = 1 | \Yv, \Xv, \Theta, z_g = 1, v_{gp} = 1) \\
%    % &\times \Pr(s_{kp} = 1 | \Yv, \Xv, \Theta, z_k = 1) \\
%    &\times \Pr(z_g = 1 | \Yv, \Xv, \Theta)
%  \end{aligned}
%\end{equation*}
%
%Note that the above expression doesn't use $\Pr(v_{gp} = 1 | \Yv, \Xv, \Theta, z_g = 1)$ in order to make the marginal posterior comparable with the models that assume all SNPs to be independent.
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\section{Posteriors on effect sizes}
%
%Also of interest are the posterior probabilities of the effect sizes per gene-SNP pair in each tissue.
%
%By maximum likelihood in each tissue separately, we can obtain the estimates of the effect sizes, $\hat{\bm{\beta}}_{gp}$, and their standard errors, recorded in the diagonal of $\hat{V}_{gp}$ (an $S \times S$ diagonal matrix).
%Using each pair of subgroups we can also fill the off-diagonal elements of $\hat{V}_{gp}$.
%
%If we now view $\hat{\bm{\beta}}_{gp}$ and $\hat{V}_{gp}$ as \emph{observations} (i.e. known), we can ``forget'' about the original data X and Y, and  write a new likelihood:
%
%$\hat{\bm{\beta}}_{gp} | \bm{\beta}_{gp} \sim \Norm_S(\bm{\beta}_{gp}, \hat{V}_{gp})$
%
%This is similar to using Laplace's method to approximate the ``real'' likelihood (i.e. the one using the original data) with a Normal centered on $\hat{\bm{\beta}}_{gp}$ and $\hat{V}_{gp}$ as covariance matrix. ($\rightarrow$ from William I think, not checked)
%
%Let's just imagine first that the prior of $\bm{\beta}_{gv}$ is not a mixture but a single Normal.
%As this prior is conjuguate to the new likelihood, the posterior simply is (see \href{http://en.wikipedia.org/wiki/Conjugate_prior#Continuous_distributions}{Wikipedia}):
%
%$\bm{\beta}_{gv} | \hat{\bm{\beta}}_{gv} \sim \Norm_S(\bm{\mu}_1, W_1)$
%
%where:
%\begin{itemize}
%\item $\bm{\mu}_1 = W_1 (\hat{V}_{gv}^{-1} \hat{\bm{\beta}}_{gv})$;
%\item $W_1 = (W_0^{-1} + \hat{V}_{gv}^{-1})^{-1}$.
%\end{itemize}
%
%Now, with the prior as a mixture, we can write the posterior as:
%\begin{align}
%p(\bm{\beta}_{gv} | \hat{\bm{\beta}}_{gv}) &= \frac{p(\bm{\beta}_{gv}) p(\hat{\bm{\beta}}_{gv} | \bm{\beta}_{gv})}{p(\hat{\bm{\beta}}_{gv})} \\
% &= \sum_{l=1}^L \lambda_l \; p_l(\bm{\beta}_{gv}) \frac{p_l(\hat{\bm{\beta}}_{gv})}{p(\hat{\bm{\beta}}_{gv})} \frac{p(\hat{\bm{\beta}}_{gv} | \bm{\beta}_{gv})}{p_l(\hat{\bm{\beta}}_{gv})} \\
% &=\sum_{l=1}^L \tilde{\lambda}_l \; \tilde{p}_l (\bm{\beta}_{gv} | \hat{\bm{\beta}}_{gv}) \\
%\end{align}
%where:
%\begin{itemize}
%\item $\tilde{\lambda}_l = \frac{\lambda_l \; p_l(\hat{\bm{\beta}}_{gv})}{p(\hat{\bm{\beta}}_{gv})}$ is the posterior probability that $\hat{\bm{\beta}}_{gv}$ belongs to component $l$;
%\item $\tilde{p}_l (\bm{\beta}_{gv} | \hat{\bm{\beta}}_{gv}) = \frac{p_l(\bm{\beta}_{gv}) p(\hat{\bm{\beta}}_{gv} | \bm{\beta}_{gv})}{p_l(\hat{\bm{\beta}}_{gv})}$ is the posterior probability of $\bm{\beta}_{gv}$ given that it is from component $l$, and it follows $\Norm_S(\bm{\mu}_{1,l}, W_{1,l})$, see above.
%\end{itemize}
%
%We estimate $\lambda_l$ via the HM, using all gene-SNP pairs, hence obtaining $\hat{\lambda}_l^{\text{HM}}$. In the empirical Bayes setting, we simply replace $\lambda_l$ above with $\hat{\lambda}_l^{\text{HM}}$.
%
%Moreover, let's note $p_l(\hat{\bm{\beta}}_{gv})$ the marginal likelihood of component $l$:
%\[
%p_l(\hat{\bm{\beta}}_{gv}) = \int p_l(\bm{\beta}_{gv}) p(\hat{\bm{\beta}}_{gv} | \bm{\beta}_{gv}) \mathsf{d} \bm{\beta}_{gv}
%\]
%
%Then, the full marginal likelihood simply is:
%\[
%p(\hat{\bm{\beta}}_{gv}) = \sum_{l=1}^L \lambda_l \; p_l(\hat{\bm{\beta}}_{gv})
%\]
%
%Therefore, in order to compute the desired posterior $p(\bm{\beta}_{gv} | \hat{\bm{\beta}}_{gv})$, I now need to know how to compute the marginal likelihood $p_l(\hat{\bm{\beta}}_{gv})$.
%
%But, as an output of the current eQtlBma package, we compute $\BF_{gvjl} = \frac{p(Y_g | X_v, \Theta, z_g=1, s_{gv}=1, c_{gvj}=1, w_{gvl}=1)}{p(Y_g | z_g=0)}$ (where $j$ indexes the configuration), and we use the ES model (i.e. $\hat{\bm{b}}$ instead of $\hat{\bm{\beta}}$).
%So there is still some work to be done to get exactly what we want.

\end{document}
