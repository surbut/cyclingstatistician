\nonstopmode  % to allow pdflatex to compile even if errors are raised (e.g. missing figures)

\documentclass[10pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}
% \graphicspath{{./figures/}} % save all figures in the same directory
\usepackage{color} 
\usepackage{hyperref}
\usepackage{parskip}
\setlength{\parindent}{0pt}

% Text layout
\topmargin 0.0cm
\oddsidemargin 0.5cm
\evensidemargin 0.5cm
\textwidth 16cm 
\textheight 21cm

%% PLEASE INCLUDE ALL MACROS BELOW
%\newcommand{\Prob}{\mathbb{P}} % symbol for discrete proba
\newcommand{\Prob}{\Pr} % symbol for discrete proba
\newcommand{\Exp}{\mathbb{E}} % symbol for expectation
\newcommand{\Var}{\mathbb{V}} % symbol for variance
\newcommand{\Norm}{{\mathcal{N}}} % symbol for Normal distribution
\newcommand{\BF}{{\text{BF}}} % symbol for Bayes factor
\newcommand{\bma}{{\BF_\text{BMA}}}
\newcommand{\bmalite}{{\BF_\text{BMAlite}}}
\newcommand{\av}{\mbox{\boldmath$\alpha$}}
\newcommand{\dv}{\bm{d}}
\newcommand{\bv}{\mbox{\boldmath$\beta$}}
\newcommand{\tauv}{\mbox{\boldmath$\tau$}}
\newcommand{\cv}{\mbox{\boldmath$c$}}
\newcommand{\bbv}{\tilde \bv}
\newcommand{\bev}{\mbox{\boldmath$b$}}
\newcommand{\ev}{\mbox{\boldmath$e$}}
\newcommand{\thv}{\mbox{\boldmath$\theta$}}
\newcommand{\tv}{\mbox{\boldmath$t$}}
\newcommand{\fv}{\mbox{\boldmath$f$}}
\newcommand{\Cv}{\mbox{\boldmath$C$}}
\newcommand{\Dv}{\mbox{\boldmath$D$}}
\newcommand{\Fv}{\mbox{\boldmath$F$}}
\newcommand{\gav}{\mbox{\boldmath$\gamma$}}
\newcommand{\Gav}{\mbox{\boldmath$\Gamma$}}
\newcommand{\Kv}{\mbox{\boldmath$K$}}
\newcommand{\iv}{\mbox{\boldmath$I$}}
\newcommand{\vv}{\mbox{\boldmath$v$}}
\newcommand{\pv}{\mbox{\boldmath$p$}}
\newcommand{\hv}{\mbox{\boldmath$h$}}
\newcommand{\gv}{\mbox{\boldmath$g$}}
\newcommand{\wv}{\mbox{\boldmath$w$}}
\newcommand{\Wv}{\mbox{\boldmath$W$}}
\newcommand{\Pv}{\mbox{\boldmath$P$}}
\newcommand{\Qv}{\mbox{\boldmath$Q$}}
\newcommand{\Rv}{\mbox{\boldmath$R$}}
\newcommand{\rv}{\mbox{\boldmath$r$}}
\newcommand{\sv}{\mbox{\boldmath$s$}}
\newcommand{\Sv}{\mbox{\boldmath$S$}}
\newcommand{\Sigv}{\mbox{\boldmath$\Sigma$}}
\newcommand{\qv}{\mbox{\boldmath$q$}}
\newcommand{\Mv}{\mbox{\boldmath$M$}}
\newcommand{\mv}{\mbox{\boldmath$\mu$}}
\newcommand{\mvg}{\mbox{\boldmath$\mu_g$}}
\newcommand{\Lv}{\mbox{\boldmath$L$}}
\newcommand{\lav}{\mbox{\boldmath$\lambda$}}
\newcommand{\Tv}{\mbox{\boldmath$T$}}
\newcommand{\Xv}{\mbox{\boldmath$X$}}
\newcommand{\xv}{\mbox{\boldmath$x$}}
\newcommand{\Uv}{\mbox{\boldmath$U$}}
\newcommand{\Vv}{\mbox{\boldmath$V$}}
\newcommand{\yv}{\mbox{\boldmath$y$}}
\newcommand{\yvg}{\mbox{\boldmath$y_g$}}
\newcommand{\Yv}{\mbox{\boldmath$Y$}}
\newcommand{\Zv}{\mbox{\boldmath$Z$}}
\newcommand{\zv}{\mbox{\boldmath$z$}}
\newcommand{\lv}{\bf{1}}
\newcommand{\etv}{\mbox{\boldmath$\eta$}}
\newcommand{\muLS}{\ensuremath{\hat{\mv}}}
\newcommand{\SigmaLS}{\ensuremath{\hat{\Sigma}}}
\newcommand{\fvPanel}{\ensuremath{\fv^{\rm panel}}}
\newcommand{\isa}{\ensuremath{\sigma_a^{-2}}}
\newcommand{\bfa}{\ensuremath{{\rm BF}}}
\newcommand{\hbfes}{\ensuremath{\widehat {\rm BF}^{\rm ES}}}
\newcommand{\hbfesmeta}{\ensuremath{\widehat {{\rm BF}}^{\rm ES}_{\rm meta}}}
\newcommand{\hbfesfix}{\ensuremath{\widehat {{\rm BF}}^{\rm ES}_{\rm fix}}}
\newcommand{\hbfesmax}{\ensuremath{\widehat {{\rm BF}}^{\rm ES}_{\rm maxH}}}
\newcommand{\hbfee}{\ensuremath{ \widehat {\rm BF}^{\rm EE} }}
\newcommand{\abfes}{\ensuremath{{\rm ABF^{ES}}}}
\newcommand{\abfee}{\ensuremath{{\rm ABF^{EE}}}}
\newcommand{\abfesc}{\ensuremath{{\rm A^*BF^{ES}}}}
%% END MACROS SECTION

\begin{document}

\title{Statistical framework from Flutre, Wen, Pritchard and Stephens (PLoS Genetics, 2013)}
\author{Timoth\'{e}e Flutre}
\date{\today}

\maketitle

\tableofcontents

\vspace{1cm}

This document describes the statistical framework with more details and sometimes a slightly different notation, notably inspired by Wen \& Stephens (Annals of Applied Statistics, 2014) and Wen (Biometrics, 2014).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Likelihood of the whole data set}

Let us imagine we measured the expression level of $G$ genes in $R$ tissues from $N$ individuals which we also genotyped at $P$ SNPs.
Note that, based on Wen's paper in Biometrics, as the tissues are sampled in the same individuals, we use $R$ for the total number of tissues instead of $S$ (in our case $s=1$).

Using $g$ to index the genes, with $g \in \{1,\dots,G\}$, we make two important assumptions about their expression levels.
We assume that the set of expression levels $\Yv_g$ of the $g$-th gene depends on the genotypes only at its $m_g$ \textit{cis} SNPs (with possibly other covariates).
We also assume that all genes are independent conditionally on these genotypes (and other covariates), which is reasonable as we focus on \textit{cis} and ignore \textit{trans} SNPs in this model.

With $\Yv = (Y_1, \dots, Y_G)$ and $\Xv = (\Xv_1, \dots, \Xv_G)$ denoting the complete set of expression levels and predictors (genotypes and other covariates) for all genes, the observed log-likelihood of the whole data set can thus be written as
\begin{equation}
  \label{lobs}
  l(\Theta; \Yv | \Xv) = \log p(\Yv | \Xv, \Theta) = \sum_{g=1}^G \log p(\Yv_g | \Xv_g, \Theta)
\end{equation}

where $\Theta$ denotes the set of all parameters detailed below.

For the moment we make no assumption as whether or not a gene is expressed in all tissues, as this can be dealt with by preprocessing (if Normal likelihood) or by directly modeling read counts (Poisson likelihood and extensions).

Also, in practice, tissue-specific confounding factors in $\Yv$ are regressed out beforehand (e.g. via PCA or factor analysis, such as in the PEER software), and the residuals, possibly quantile-normalized (if Normal likelihood), are used as responses.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Focus on a single gene-SNP pair}

The likelihood for gene $g$ and SNP $p$ is:
\begin{equation}
Y_g | X_p, B_{gp}, X_c, B_{gc}, \Sigma_{gp} \sim \Norm_{N \times R}(X_p B_{gp} + X_c B_{gc}, I_N, \Sigma_{gp})
\end{equation}

where:
\begin{itemize}
\item $Y_g$ is the $N \times R$ matrix of expression levels;
\item $X_p$ is the $N \times 1$ matrix of genotypes (assuming the same individuals in all tissues);
\item $B_{gp}$ is the unknown $1 \times R$ matrix of genotype effect sizes;
\item $X_c$ is the $N \times (1+Q)$ matrix of known covariates (including a column of 1's for the intercepts);
\item $B_{gc}$ is the unknown $(1+Q) \times R$ matrix of covariate effect sizes (including the $\mu_s$);
\item $\Norm_{N \times R}$ is the \href{https://en.wikipedia.org/wiki/Matrix_normal_distribution}{matrix Normal distribution};
\item $\Sigma_{gp}$ is the unknown $R \times R$ covariance matrix of the errors.
\end{itemize}

For mathematical convenience (especially in the case of multiple SNPs), we vectorize the rows of $B_{gp}$ into $\bm{\beta}_{gp}$.
Here, as we focus on one SNP at a time, we directly have $\bm{\beta}_{gp} = B_{gp}^T$.

To make inference invariant to linear transformations of the response variables (see Servin \& Stephens, 2007), we preferentially model the standardized genotype effect sizes:
\[
\forall r \; b_{gpr} = \beta_{gpr} / \sigma_{gpr}
\]
where $\beta_{gpr}$ is the $r$-th element in $\bm{\beta}_{gp}$ and $\sigma_{gpr}$ is the $r$-th element on the diagonal of $\Sigma_{gp}$.

We use the notion of \emph{configuration}, a latent indicator $R$-dimensional vector $\bm{\gamma}_{gp}$ such that $\gamma_{gpr} = 1$ means the eQTL is active in tissue $r$, i.e. $b_{gpr} \ne 0$, whereas $\gamma_{gpr} = 0$ means the eQTL is inactive in tissue $r$, i.e. $b_{gpr} = 0$.
Moreover, we introduce an unknown mean $\bar{b}_{gp}$, to finally get the following ``spike-and-slab'' prior allowing to borrow information across tissues in which the eQTL is active:
\begin{equation}
  b_{gpr} | \gamma_{gpr}, \bar{b}_{gp}, \phi \sim \gamma_{gpr} \Norm(\bar{b}_{gp}, \phi^2) + (1 - \gamma_{gpr}) \delta_0
\end{equation}
where $\delta_0$ is a point mass at 0, and
\begin{equation}
  \bar{b}_{gp} | \omega \sim \Norm(0, \omega^2)
\end{equation}

Whereas the configuration handles qualitative heterogeneity (having an effect or not), the hyperparameters $\phi$ and $\omega$ handles quantitative heterogeneity (having possibly different, non-null effects).
By integrating out $\bar{b}_{gp}$, we can see that $\phi^2 + \omega^2$ controls the average magnitude of the effect in any tissue  and $\phi^2/(\omega^2+\phi^2)$ controls the amount of heterogeneity.

Equivalently, we can write this prior as a multivariate Normal:
\begin{equation}
  \bm{b}_{gp} | U_{gp0} \sim \Norm_R(\bm{0}, U_{gp0})
\end{equation}
where, following Wen (2014), $U_{gp0}$ is parametrized as $(\Gamma_{gp},\Delta_{gp})$:
\begin{equation}
  p(U_{gp0}) = p(\Delta_{gp} | \Gamma_{gp}) \Prob(\Gamma_{gp})
\end{equation}
so that $\Gamma_{gp}$ is a binary matrix consisting of entry-wise non-zero indicators and is identical in size and layout to $U_{gp0}$, and $\Delta_{gp}$ is an indexed set of numerical values quantifying each non-zero entry in $\Gamma_{gp}$.
The skeleton $\Gamma_{gp}$ has $\bm{\gamma}_{gp}$ on the diagonal. Each off-diagonal entry $\Gamma_{gp,ij}$ is equal to 1 has long as diagonal elements $\Gamma_{gp,ii}$ and $\Gamma_{gp,jj}$ are both equal to 1.

In the current application, we choose:
\begin{equation}
  U_{gp0} \, | \, \bm{\gamma}_{gp} = \bm{1}, \phi, \omega \; \; =
  \begin{pmatrix}
    \phi^2 + \omega^2 & \cdots & \omega^2 \\
    \vdots & \ddots & \vdots \\
    \omega^2 & \cdots & \phi^2 + \omega^2
  \end{pmatrix}
\end{equation}
In terms of notation, the $0$ in $U_{gp0}$ indicates the prior. See section on posterior effect sizes for $U_{gp1}$.

In practice, we use a known grid for prior variances, i.e. $L$ pairs of values $(\phi_l,\omega_l)$ leading to a mixture of multivariate Normals:
\begin{equation}
  \label{prior_b_mixt}
  \bm{b}_{gp} | \bm{\lambda},\Uv_{gp0} \sim \sum_{l=1}^L \lambda_l \; \Norm_R(\bm{0}, U_{gp0l})
\end{equation}

See Wen \& Stephens (2014)  and Wen (2014) for the priors on the other parameters, as well as the analytical approximation (Laplace method) to calculate the Bayes factor testing any active configuration against the global null.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hierarchical model for all gene-SNP pairs}

In order to detect eQTLs and identify in which tissue(s) they are active, we present in this document a generative model for the whole data set.
This model is hierarchical with explicit gene and SNP levels.
Moreover, it borrows information between tissues as well as between genes.

\bigskip

For gene $g$, we use a latent binary indicator $z_g$ to denote if there is any eQTL in its \textit{cis}-region for at least one tissue:
\begin{equation}
  \Prob(z_g = 1) = 1 - \pi_0.
\end{equation}
In Flutre \textit{et al}, permutations were used to estimate this parameter, $\pi_0$, via Storey's method as implemented in the \verb+qvalue+ package in $\mathsf{R}$. Note that the EBF and QBF procedures from Wen (arXiv 2013) can also be used.

\bigskip

We use a latent indicator $m_g$-dimensional vector $\vv_g$ to denote the true eQTL (i.e. ``the'' eQT\emph{N}) conditional on $z_g=1$, and let $v_{gp}$ denote the $p$-th entry of $\vv_g$ ($p \in \{1,\dots,m_g\}$).
To be more specific, we attempt here to find eQTNs rather than eQTLs.
In Flutre \textit{et al}, the ``one \textit{cis} eQTL per gene'' assumption restricts $\vv_g$ to have at most one entry equal to 1, with the remaining entries being 0.
See Wen (Biometrics, 2014) for the variable selection model relaxing this assumption (but only for fine mapping as it uses MCMC).
Accordingly,
\begin{equation*}
  \Prob(\vv_g = \bm{0}  | z_g = 0) = 1,
\end{equation*}
and
\begin{equation}
  \Prob(v_{gp} = 1  | z_g = 1)  =  \nu_{p},
\end{equation}
for which we also make the simplifying assumption that $\nu_p = \frac{1}{m_g}$, i.e. all \textit{cis} SNPs are equally likely, \emph{a priori}, to be the eQTL for gene $g$.
But see Veyrieras \textit{et al} to parametrize $\nu_p$ in terms of genomic annotations (although in the single-tissue case).

\bigskip

We use a latent indicator $J$-dimensional vector $\cv_{gp}$ to denote the actual configuration, where $J=2^R-1$.
In case the SNP is not an eQTL,
\begin{equation}
  \Prob(\cv_{gp} = \bm{0}  | v_{gp} = 0) = 1.
\end{equation}
Otherwise, we assume the gene-SNP pair to be in the $j$-th configuration with prior probability
\begin{equation}
  \Prob(c_{gpj} = 1 | v_{gp} = 1) = \eta_j
\end{equation}
with the constraints $\forall j \; \eta_j \ge 0$ and $\sum_j \eta_j = 1$.
All column vectors $\cv_{gp}$ for all $m_g$ SNPs are gathered into the set $C_g$.

\bigskip

Finally, based on a grid of $L$ pairs of values $(\omega_l^2,\phi_l^2)$, we use a latent $L$-dimensional vector $\dv_{gp}$ to indicate the actual pair of variances for the prior on the standardized genotype effect sizes.
The $l$-th entry of this indicator vector is denoted by $d_{gpl}$, for which we assume prior probability
\begin{equation}
  \Prob(\dv_{gp}= \bm{0} | v_{gp} = 0) =  1
\end{equation}
and
\begin{equation}
  \Prob(d_{gpl}=1 | v_{gp} = 1 ) =  \lambda_l
\end{equation}
with the constraints $\forall l \; \lambda_l \ge 0$ and $\sum_l \lambda_l = 1$.
All column vectors $\dv_{gp}$ for all $m_g$ SNPs are gathered into the set $D_g$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Augmented likelihood}

In the maximum likelihood framework, for any gene $g$, we treat latent variables $z_g, \vv_g, C_g$ and $D_g$ as missing data.
Let $\zv = (z_1,\dots,z_G)$, $\Vv = (\vv_1,\dots,\vv_G)$, $\Cv = (C_1,\dots,C_G)$ and $\Dv = (D_1,\dots,D_G)$ denote the complete collection of latent variables.

Let $\Theta=(\pi_0,\eta_1,\dots,\eta_K,\lambda_1,\dots,\lambda_L)$ denote the set of parameters.
Based on the hierarchical model described in the previous section, we can now write out the augmented log-likelihood as follows,

\begin{equation*}
  \begin{aligned}
    l_a(\Theta;\Yv,\zv,\Vv,\Cv,\Dv|\Xv) &= \sum_g \log p(Y_g,z_g,\vv_g,C_g,D_g | \Xv_g,\Theta)
  \end{aligned}
\end{equation*}

Expanding the term inside the sum:
\begin{equation*}
  \begin{aligned}
    \log p(Y_g,z_g,\vv_g,C_g,D_g | \Xv_g,\Theta) &= \log p(z_g | \Theta) + \log p(Y_g,\vv_g,C_g,D_g | \Xv_g,\Theta,z_g)\\
    &= (1-z_g) \log \pi_0 + z_g \log (1-\pi_0) \\
    &+ (1-z_g) \log p(Y_g|\Xv_g,z_g=0) + z_g \log p(Y_g,\vv_g,C_g,D_g | \Xv_g,\Theta,z_g=1) \\
    &= (1-z_g) \log \pi_0 + z_g \log (1-\pi_0) + \log p(Y_g|\Xv_g,z_g=0) \\
    &+ z_g \log \frac{p(Y_g,\vv_g,C_g,D_g | \Xv_g,\Theta,z_g=1)}{p(Y_g|\Xv_g,z_g=0)}
  \end{aligned}
\end{equation*}

The ratio corresponds to the Bayes factor for gene $g$:
\begin{equation*}
  \begin{aligned}
    \log \BF_g &= \log \frac{p(Y_g,\vv_g,C_g,D_g | \Xv_g,\Theta,z_g=1)}{p(Y_g|\Xv_g,z_g=0)} \\
    &= \sum_p v_{gp} \log \nu_p + \sum_p v_{gp} \log \frac{p(Y_g,\cv_{gp},\dv_{gp} | \Xv_{gp},\Theta,v_{gp}=1)}{p(Y_g|\Xv_g,z_g=0)}
  \end{aligned}
\end{equation*}

The ratio inside the sum corresponds to the Bayes factor for gene $g$ and SNP $p$:
\begin{equation*}
  \begin{aligned}
    \log \BF_{gp} &= \log \frac{p(Y_g,\cv_{gp},\dv_{gp} | \Xv_{gp},\Theta,v_{gp}=1)}{p(Y_g|\Xv_g,z_g=0)} \\
    &= \sum_j c_{gpj} \log \eta_j + \sum_l d_{gpl} \log \lambda_l \\
    &+ \sum_j \sum_l c_{gpj} d_{gpl} \log \frac{p(Y_g | \Xv_{gp},\Theta,c_{gpj}=1,d_{gpl}=1)}{p(Y_g|\Xv_g,z_g=0)}
  \end{aligned}
\end{equation*}

The last ratio corresponds to the Bayes factor for gene $g$ and SNP $p$ in configuration $j$ with prior variances $l$, and can be analytically approximated as described in Wen (2014):
\begin{equation*}
  \BF_{gpjl} = \frac{p(Y_g | \Xv_{gp},\Theta,c_{gpj}=1,d_{gpl}=1)}{p(Y_g|\Xv_g,z_g=0)}  
\end{equation*}

Putting everything back together:
\begin{equation}
 \label{laug}  
  \begin{aligned}
    l_a(\Theta;\Yv,\zv,\Vv,\Cv,\Dv|\Xv) &= \sum_g (1-z_g)\log \pi_0 + \sum_g z_g \log (1-\pi_0) + \sum_g \log p(Y_g|\Xv_g,z_g=0) \\
    &+ \sum_{g,p} z_gv_{gp} \log \frac{1}{m_g} + \sum_{g,p,j}z_g v_{gp} c_{gpj} \log \eta_j + \sum_{g,p,l} z_g v_{gp} d_{gpl} \log \lambda_l  \\
    &+ \sum_{g,p,j,l} z_g v_{gp} c_{gpj} d_{gpl} \log \BF_{gpjl}
  \end{aligned}
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{EM algorithm}

The EM algorithm searches for the maximum likelihood estimate of $\Theta$, by iteratively performing an expectation (E) step and a maximization (M) step of the following objective function, noted $Q$:
\begin{equation}
  \label{Q}
  \begin{aligned}
    Q(\Theta|\Yv,\Xv,\Theta^{(i)}) = \Exp_{\zv,\Vv,\Cv,\Dv|\Yv,\Xv,\Theta} [ l_a(\Theta) | \Yv,\Xv,\Theta^{(i)} ]
  \end{aligned}
\end{equation}

Starting from randomly-initialized parameters $\Theta^{(0)}$, in the E-step for the $(i+1)^{\text{th}}$ iteration, we evaluate the objective function (\ref{Q}), i.e. the conditional expectation over the latent variables of the augmented log-likelihood (\ref{laug}) given the observed data $\Xv$ and $\Yv$ and the current estimates of the parameters $\Theta^{(i)}$.

\begin{equation}
  \begin{aligned}
    \Exp[z_g | \Yv, \Xv, \Theta^{(i)}] &= \Prob(z_g=1 | \Yv, \Xv,\Theta^{(i)}) \\ 
    &= \frac{\Prob(z_g=1 | \Theta^{(i)}) \; p(\Yv | \Xv, \Theta^{(i)}, z_g=1)}{p(\Yv | \Xv, \Theta^{(i)})} \\
    &= \frac{\Prob(z_g=1 | \Theta^{(i)}) \; \prod_{g'} p(\Yv_{g'} | \Xv_{g'}, \Theta^{(i)}, z_g=1)}{\prod_{g'} p(\Yv_{g'} | \Xv_{g'}, \Theta^{(i)})} \\
    &= \frac{\Prob(z_g=1 | \Theta^{(i)}) \; p(Y_g | \Xv_g, \Theta^{(i)}, z_g=1)}{p(Y_g | \Xv_g, \Theta^{(i)})} \\
    &= \frac{(1-\pi_0^{(i)}) \BF^{(i)}_{g}}{\pi_0^{(i)} + (1-\pi_0^{(i)}) \BF^{(i)}_{g}}.
  \end{aligned}
\end{equation}

where 
\begin{equation*}
  \begin{aligned}
    \BF^{(i)}_{g} &= \frac{p(Y_g | \Xv_g, \Theta^{(i)}, z_g=1)}{p(Y_g | \Xv_g, z_g=0)} \\
    &= \sum_{p,j,l} \frac{1}{m_g} \eta_j^{(i)} \lambda_l^{(i)} \BF_{gpjl}.
  \end{aligned}
\end{equation*}

Similarly,
\begin{equation}
  \begin{aligned}
    \Exp[z_g v_{gp} | \Yv, \Xv, \Theta^{(i)}] &= \frac{(1-\pi_0^{(i)}) \frac{1}{m_g} \BF^{(i)}_{gp}}{\pi_0^{(i)} + (1-\pi_0^{(i)}) \BF^{(i)}_{g}},
  \end{aligned}
\end{equation}

where 
\begin{equation*}
  \begin{aligned}
    \BF^{(i)}_{gp} &= \frac{p(Y_g | \Xv_{gp}, \Theta^{(i)}, z_g=1, v_{gp}=1)}{p(Y_g | \Xv_g, z_g=0)} \\
    &=\sum_{j,l} \eta_j^{(i)} \lambda_l^{(i)} \BF_{gpjl}.
  \end{aligned}
\end{equation*}

And
\begin{equation}
  \begin{aligned}
    \Exp[z_g v_{gp} c_{gpj} | \Yv, \Xv, \Theta^{(i)}] &= \frac{(1-\pi_0^{(i)}) \frac{1}{m_g} \eta_j^{(i)} \sum_l \lambda_l^{(i)} \BF_{gpjl}}{\pi_0^{(i)} + (1-\pi_0^{(i)}) \BF^{(i)}_{g}},
  \end{aligned}
\end{equation}

\begin{equation}
  \begin{aligned}
    \Exp[z_g v_{gp} d_{gpl} | \Yv, \Xv, \Theta^{(i)}] &= \frac{(1-\pi_0^{(i)}) \frac{1}{m_g} \lambda_l^{(i)} \sum_j \eta_j^{(i)} \BF_{gpjl}}{\pi_0^{(i)} + (1-\pi_0^{(i)}) \BF^{(i)}_{g}}.
  \end{aligned}
\end{equation}


In the M-step for the $(i+1)^{\text{th}}$ iteration, we estimate a new set of parameters, $\Theta^{(i+1)}$, by maximizing the objective function (\ref{Q}), i.e. the conditional expectation over the latent variables of the augmented log-likelihood (\ref{laug}) given the observed data $\Xv$ and $\Yv$ and the current estimates of the parameters $\Theta^{(i)}$.

In particular, for $\pi_0$,
\begin{equation*}
  \begin{aligned}
    \frac{\partial Q}{\partial \pi_0}(\pi_0) &= \sum_g \left[ (1-\Exp[z_g | \Yv, \Xv, \Theta^{(i)}]) \times \frac{1}{\pi_0} \right] - \sum_g \left[ \Exp[z_g | \Yv, \Xv, \Theta^{(i)}] \times \frac{1}{1-\pi_0} \right],
  \end{aligned}
\end{equation*}

\begin{equation*}
  \begin{aligned}
    \frac{\partial Q}{\partial \pi_0}(\pi_0^{(i+1)}) = 0 \Leftrightarrow \pi_0^{(i+1)} = \frac{1}{G} \sum_g (1 - \Exp[z_g | \Yv, \Xv, \Theta^{(i)}]),
  \end{aligned}
\end{equation*}

which gives
\begin{equation}
  \begin{aligned}
    \pi_0^{(i+1)} = \frac{1}{G} \sum_g \frac{\pi_0^{(i)}}{\pi_0^{(i)} + (1-\pi_0^{(i)}) \BF_g^{(i)}}.
  \end{aligned}
\end{equation}

Now for the grid points, using a Lagrange multiplier, $L_a$, to enforce the constraints,
\begin{equation*}
  \begin{aligned}
    \frac{\partial Q}{\partial \lambda_l}(\lambda_l) &= \sum_{g,p} \left[ \Exp[z_g v_{gp} d_{gpl} | \Yv, \Xv, \Theta^{(i)}] \times \frac{1}{\lambda_l} \right] - L_a,
  \end{aligned}
\end{equation*}

\begin{equation*}
  \begin{aligned}
    \frac{\partial Q}{\partial \lambda_l}(\lambda_l^{(i+1)}) = 0 \Leftrightarrow \lambda_l^{(i+1)} = \frac{1}{L_a} \sum_{g,p} \Exp[z_g v_{gp} d_{gpl} | \Yv, \Xv, \Theta^{(i)}],
  \end{aligned}
\end{equation*}

which gives
\begin{equation}
  \begin{aligned}
    \lambda_l^{(i+1)} = \frac{\sum_{g,p,j} \frac{\frac{1}{m_g} \eta_j^{(i)} \BF_{gpjl}}{\pi_0^{(i)} + (1-\pi_0^{(i)}) \BF_g^{(i)}} \; \lambda_l^{(i)}}{\sum_{l'} \left( \sum_{g,p,j} \frac{\frac{1}{m_g} \eta_j^{(i)} \BF_{gpjl'}}{\pi_0^{(i)} + (1-\pi_0^{(i)}) \BF_g^{(i)}} \; \lambda_{l'}^{(i)} \right)}.
  \end{aligned}
\end{equation}

where the $(1-\pi_0^{(i)})$ simplified but the $1/m_g$ are kept as the SNP prior could easily become SNP-specific via the usage of external information such as genome annotations.

Finally, for the configurations,
\begin{equation}
  \begin{aligned}
    \eta_j^{(i+1)} = \frac{\sum_{g,p,l} \frac{\frac{1}{m_g} \lambda_l^{(i)} \BF_{gpjl}}{\pi_0^{(i)} + (1-\pi_0^{(i)}) \BF_g^{(i)}} \; \eta_j^{(i)}}{\sum_{j'} \left( \sum_{g,p,l} \frac{\frac{1}{m_g} \lambda_l^{(i)} \BF_{gpjl}}{\pi_0^{(i)} + (1-\pi_0^{(i)}) \BF_g^{(i)}} \; \eta_{j'}^{(i)} \right)}.
  \end{aligned}
\end{equation}


We initiate the EM algorithm by setting $\Theta^{(0)}$ to some initial values, random or not, and run iterations until some pre-defined convergence threshold is met.
In practice, we monitor the monotonic increase of the observed log-likelihood (\ref{lobs}) between successive iterations, as guaranteed by the EM algorithm, and stop as the increment becomes sufficiently small (e.g. 0.05).

We can also construct confidence intervals for $\hat{\Theta}$ using the profile likelihood.
For example, a $(1-\alpha)\%$ profile likelihood confidence set for $\pi_0$ is built as 
\begin{equation}
  \{\pi_0: \log p(\Yv | \Xv, \pi_0, \hat \etv, \hat \lav) > \log p(\Yv | \Xv, \hat \pi_0, \hat \etv, \hat \lav) - \frac{1}{2}Z^2_{(1-\alpha)} \},
\end{equation}
where $\hat \pi_0, \hat \etv, \hat \lav$ are the MLEs obtained from the EM algorithm. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Posteriors on latent variables}

Once the hyperparameters have been estimated, several posteriors of interest can be calculated.
The first is the posterior probability that the gene has one eQTL in at least one tissue:
\begin{equation}
  \begin{aligned}
    \Prob(z_g = 1 | \Yv, \Xv, \Theta) &= \frac{\Prob(z_g=1 | \Theta) \; p(\Yv | \Xv, \Theta, z_g=1)}{p(\Yv | \Xv, \Theta)} \\
    &= \frac{(1-\hat{\pi}_0) \BF_g}{\hat{\pi}_0 + (1-\hat{\pi}_0) \BF_g}
  \end{aligned}
\end{equation}

Note that the $\BF_g$'s alone can be used in order to estimate $\pi_0$ via the EBF procedure (Wen, arXiv 2013).

At the SNP level, the posterior probability that the SNP is ``the'' eQTL for the gene, in at least one tissue, given that the gene has exactly one eQTL, assuming all cis SNPs are equally likely, is given by:
\begin{equation}
  \begin{aligned}
    \Prob(v_{gp} = 1 | \Yv, \Xv, \Theta, z_g = 1) &= \frac{\frac{1}{m_g} p(Y_g | X_g, \Theta, z_g=1, v_{gp}=1)}{p(Y_g | X_g, \Theta, z_g = 1)} \\
    &= \frac{\frac{1}{m_g} \BF_{gp}}{\BF_g} = \frac{\BF_{gp}}{\sum_p \BF_{gp}}
  \end{aligned}
\end{equation}

In order to make results more comparable with models only considering the SNP level, the posterior probability that the SNP is ``an'' eQTL for the gene, in at least one tissue, given that all SNPs are independent (and effects are small), can be written as:
\begin{equation}
  \begin{aligned}
    \Prob(v_{gp} = 1 | \Yv, \Xv, \Theta, z'_g \ge 1) &= \frac{\frac{1}{m_g} \BF_{gp}}{\frac{1}{m_g} \BF_{gp} + (1-\frac{1}{m_g})}
  \end{aligned}
\end{equation}

We are also interested in the posterior on configurations:
\begin{equation}
  \begin{aligned}
    \Prob(c_{gpj} = 1 | \Yv, \Xv, \Theta, z_g = 1, v_{gp} = 1) &= \frac{\hat{\eta}_j \sum_l \hat{\lambda}_l \BF_{gpjl}}{\BF_{gp}}
  \end{aligned}
\end{equation}

But of most interest is the posterior probability that the SNP is an eQTL in a given tissue, given that it is ``the'' eQTL for the gene:
\begin{equation}
  \begin{aligned}
    \Prob(\gamma_{gpr} = 1 | \Yv, \Xv, \Theta, z_g = 1, v_{gp} = 1) &= \sum_{j:\gamma_{gpr}=1|c_{gpj}} \Prob(c_{gpj} = 1 | \Yv, \Xv, \Theta, z_g = 1, v_{gp} = 1) \\
    &= \sum_{j:\gamma_{gpr}=1 | c_{gpj}} \frac{\hat{\eta}_j \sum_l \hat{\lambda}_l \BF_{gpjl}}{\sum_{j',l} \hat{\eta}_{j'} \hat{\lambda}_l \BF_{gpj'l}}
  \end{aligned}
\end{equation}

When reporting the results, it is usually better to report the following marginal posterior:
\begin{equation}
  \begin{aligned}
    \Prob(\gamma_{gpr} = 1 | \Yv, \Xv, \Theta) &= \Prob(\gamma_{gpr} = 1 | \Yv, \Xv, \Theta, z_g = 1, v_{gp} = 1) \\
    % &\times \Prob(v_{gp} = 1 | \Yv, \Xv, \Theta, z_g = 1) \\
    &\times \Prob(z_g = 1 | \Yv, \Xv, \Theta)
  \end{aligned}
\end{equation}

Note that the above expression doesn't use $\Prob(v_{gp} = 1 | \Yv, \Xv, \Theta, z_g = 1)$ in order to make the marginal posterior comparable with the models that assume all SNPs to be independent (e.g. Li \textit{et al}, arXiv 2013).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Posteriors on genotype effect sizes}

Also of interest are the posterior probabilities of the genotype effect sizes per gene-SNP pair in each tissue.

By maximum likelihood in each tissue separately, we can easily obtain the estimates of the standardized genotype effect sizes, $\hat{\bm{b}}_{gp}$, and their standard errors recorded on the diagonal of an $R \times R$ matrix noted $\hat{V}_{gp} = \Var(\hat{\bm{b}}_{gp})$.
Using each pair of tissues, we can also fill the off-diagonal elements of $\hat{V}_{gp}$.

If we now view $\hat{\bm{b}}_{gp}$ and $\hat{V}_{gp}$ as \emph{observations} (i.e. known), we can ``forget'' about the original data $X_p,X_c$ and $Y_g$, and  write a new ``likelihood'' (using only the sufficient statistics):

$\hat{\bm{b}}_{gp} | \bm{b}_{gp} \sim \Norm_R(\bm{b}_{gp}, \hat{V}_{gp})$

% This is similar to using Laplace's method to approximate the ``real'' likelihood (i.e. the one using the original data) with a Normal centered on $\hat{\bm{b}}_{gp}$ and $\hat{V}_{gp}$ as covariance matrix. ($\rightarrow$ from William I think, not checked)

Let us imagine first that the prior on $\bm{b}_{gp}$ is not a mixture but a single Normal: $\bm{b}_{gp} \sim \Norm_R(\bm{0}, U_{gp0})$.
As this prior is conjuguate to the ``likelihood'' above, the posterior simply is (see \href{http://en.wikipedia.org/wiki/Conjugate_prior#Continuous_distributions}{Wikipedia}):

$\bm{b}_{gp} | \hat{\bm{b}}_{gp} \sim \Norm_R(\bm{\mu}_{gp1}, U_{gp1})$

where:
\begin{itemize}
\item $\bm{\mu}_{gp1} = U_{gp1} (\hat{V}_{gp}^{-1} \hat{\bm{b}}_{gp})$;
\item $U_{gp1} = (U_{gp0}^{-1} + \hat{V}_{gp}^{-1})^{-1}$.
\end{itemize}

In practice, we use a mixture for prior (see \ref{prior_b_mixt}):
\begin{equation}
  \bm{b}_{gp} | \bm{\lambda}, \Uv_{gp0j} \sim \sum_{l=1}^L \lambda_l \; \Norm_R(\bm{0}, U_{gp0jl})
\end{equation}
for which the hyper-parameters are either fixed ($\Uv_{gp0j}$) or estimated ($\bm{\lambda}$) as described above using the full hierarchical model and the EM algorithm.
Moreover, the posteriors we may want are $\bm{b}_{gp} | \hat{\bm{b}}_{gp}, \hat{V}_{gp}, \hat{\Theta}$ (full marginal) or $\bm{b}_{gp} | \hat{\bm{b}}_{gp}, \hat{V}_{gp}, \hat{\Theta}_{-\pi_0}, v_{gp}=1$ (conditional on being the eQTN in at least one tissue, averaging over all configurations and whole grid) or $\bm{b}_{gp} | \hat{\bm{b}}_{gp}, \hat{V}_{gp}, \hat{\bm{\lambda}}, \gamma_{gpr}=1$ (conditional on being an active eQTN in tissue $r$, averaging over all configurations in which this tissue is active and whole grid) or $\bm{b}_{gp} | \hat{\bm{b}}_{gp}, \hat{V}_{gp}, \hat{\bm{\lambda}}, c_{gpj}=1$ (conditional on being the eQTN and in configuration $j$, averaging over whole grid).

Let us start with the latter:
\begin{equation}
  \begin{aligned}
    p(\bm{b}_{gp} | \hat{\bm{b}}_{gp}, \hat{V}_{gp}, \hat{\bm{\lambda}}, c_{gpj}=1) &= \frac{p(\bm{b}_{gp} | \hat{\bm{\lambda}}, \Uv_{gp0j}) \; p(\hat{\bm{b}}_{gp} | \bm{b}_{gp}, \hat{V}_{gp})}{p(\hat{\bm{b}}_{gp} | \hat{V}_{gp}, \hat{\bm{\lambda}})} \\
    &= \sum_{l=1}^L \hat{\lambda}_l \; p_l(\bm{b}_{gp}) \frac{p_l(\hat{\bm{b}}_{gp})}{p(\hat{\bm{b}}_{gp} | \hat{V}_{gp}, \hat{\bm{\lambda}})} \frac{p(\hat{\bm{b}}_{gp} | \bm{b}_{gp}, \hat{V}_{gp})}{p_l(\hat{\bm{b}}_{gp})} \\
    &=\sum_{l=1}^L \tilde{\lambda}_l \; \tilde{p}_l (\bm{b}_{gp} | \hat{\bm{b}}_{gp}) \\
  \end{aligned}
\end{equation}
where:
\begin{itemize}
\item $p_l(\hat{\bm{b}}_{gp})$ corresponds to the marginal distribution of $\hat{\bm{b}}_{gp} | d_{gpl}=1$;
\item $\tilde{\lambda}_l = \hat{\lambda}_l \frac{p_l(\hat{\bm{b}}_{gp})}{p(\hat{\bm{b}}_{gp})}$ is the probability that $\hat{\bm{b}}_{gp}$ belongs to component $l$;
\item $\tilde{p}_l (\bm{b}_{gp} | \hat{\bm{b}}_{gp}) = \frac{p_l(\bm{b}_{gp}) p(\hat{\bm{b}}_{gp} | \bm{b}_{gp})}{p_l(\hat{\bm{b}}_{gp})}$ is the posterior probability of $\bm{b}_{gp}$ given that it is from component $l$.
\end{itemize}

Using the law of total expectations (as done by Sarah), we get:
\begin{equation}
  \Exp[\hat{\bm{b}}_{gp} | d_{gpl}=1] = \Exp[\Exp[\hat{\bm{b}}_{gp} | d_{gpl}=1, \bm{b}_{gp}]] = \Exp[\bm{b}_{gp} | d_{gpl}=1] = 0
\end{equation}
and using the law of total variances:
\begin{equation}
  \Var[\hat{\bm{b}}_{gp} | d_{gpl}=1] = \Var[\Exp[\hat{\bm{b}}_{gp} | d_{gpl}=1, \bm{b}_{gp}]] + \Exp[\Var[\hat{\bm{b}}_{gp} | d_{gpl}=1, \bm{b}_{gp}]] = U_{gp0l} + \hat{V}_{gp}
\end{equation}
Therefore the marginal is $\hat{\bm{b}}_{gp} | d_{gpl}=1 \sim \Norm_R(\bm{0}, U_{gp0l} + \hat{V}_{gp})$.

\subsection{Computing the Full Joint Posterior on $\emph{b}_{gp}$}

We recognize that the marginal is $\hat{\bm{b}}_{gp} | d_{gpl}=1 \sim \Norm_R(\bm{0}, U_{gp0l} + \hat{V}_{gp})$.
Therefore, 

\begin{equation}
  \begin{aligned} 
  p(\bm{b}_{gp} | \hat{\bm{b}}_{gp}, \hat{V}_{gp}, \hat{\bm{\lambda}}, c_{gpj}=1)  &= \sum_{l=1}^L \tilde{\lambda}_l \frac{p_l(\bm{b}_{gp}) p(\hat{\bm{b}}_{gp} | \bm{b}_{gp})}{p_l(\hat{\bm{b}}_{gp})}\\
      &= \sum_{l=1}^L \hat{\lambda}_l \frac{p_l(\hat{\bm{b}}_{gp})}{p(\hat{\bm{b}}_{gp})} \frac{p_l(\bm{b}_{gp}) p(\hat{\bm{b}}_{gp} | \bm{b}_{gp})}{p_l(\hat{\bm{b}}_{gp})}\\
      &= \sum_{l=1}^L \hat{\lambda}_l \frac{p_l(\bm{b}_{gp}) p(\hat{\bm{b}}_{gp} | \bm{b}_{gp})}{{p(\hat{\bm{b}}_{gp})}} \\
      &=  \sum_{l=1}^L \frac {\hat{\lambda}_l  \Norm_R(\hat{\bm{b}}_{gp}|\bm{0}, U_{gp0l} + \hat{V}_{gp})  \Norm_R(\bm{b}_{gp}|\bm{\mu}_{gp1jl}, U_{gp1jl})} {\sum_{l=1}^L \hat{\lambda}_l \Norm_R(\hat{\bm{b}}_{gp}|\bm{0}, U_{gp0l} + \hat{V}_{gp})}
       \end{aligned} 
       \end{equation}
 
where:
\begin{itemize}
\item $\bm{\mu}_{gp1jl} = U_{gp1jl} (\hat{V}_{gp}^{-1} \hat{\bm{b}}_{gp})$;
\item $U_{gp1jl} = (\mathbf{I} +U_{gp0jl} \hat{V}^{-1})^{-1} U_{gp0jl}$.


\end{itemize}

Thus we require the summation over a variety of configuration weights conditional being the eQTN in configuration $\emph{j}$

%TODO (Tim: Sarah, you can start writing below. You should not need to edit anything above. In the end, we should have something ready to be implemented in R using output from eqtlbma ;)

\end{document}
